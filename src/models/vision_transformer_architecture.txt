Cross-Attention Vision Transformer (ViT-CoAttention)
===========================================================

Architecture Diagram:
---------------------

                                 Input Image (224x224) (with augmented data)
                                          │
                                [ ViT-B/16 Backbone ]
                                          │
                                    [CLS] Token 
                                   (Batch, 768)
                                          │
                  ┌───────────────────────┴────────────────────────┐
                  │ (Split)                                        │ (Split)
                  ▼                                                ▼
         [ Super MLP Stream ]                             [ Sub MLP Stream ] 
             (768 -> 512)                                     (768 -> 512)
                  │                                                │
                  │ Super Features                                 │ Sub Features
                  │ (KEY & VALUE)                                  │ (QUERY)
                  │                                                │
                  │                                                │
                  └───────────────► [ Cross-Attention ] ◄──────────┘
                                    Attn(Q=Sub, K=Super, V=Super)
                                           │
                                           │ Attention Output
                                           ▼
                                 [ Residual Connection ]
                                 Sub = Sub + Attention
                                           │
                                           │ Refined Sub Features
                                           ▼
         [ Super Classifier ]     [ Sub Classifier ]
             (512 -> 4)               (512 -> 88)
                  │                        │
            Superclass Preds         Subclass Preds
            (no attention)           (attention)
